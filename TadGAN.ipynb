{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TadGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python [conda env:tf22_kr24]",
      "language": "python",
      "name": "conda-env-tf22_kr24-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVhHscIMEtMz"
      },
      "source": [
        "### TadGAN for Tensorflow 2.0 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvvVAq07YKLP"
      },
      "source": [
        "#### Part 1\n",
        "- Connect and authenticate user google drive \n",
        "- Data load and prepare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_TAcBTAVPOj"
      },
      "source": [
        "# drive mount \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')#, force_remount=True)  # Force_remount 는 강제적으로 해당 경로로 mount 하겠다는 것입니다. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8C3rTcZVgzM"
      },
      "source": [
        "os.chdir('/content/drive/My Drive/CoLab/TimeSeries/TadGAN') # 다음 python 실행 부터는 해당 코드만 실행하면 됩니다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R71s6TR4U72u"
      },
      "source": [
        "# load generals\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from utils import plot, plot_ts, plot_rws, plot_error, unroll_ts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKNr2O4aYT5m"
      },
      "source": [
        "df = pd.read_csv('nyc_taxi.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBg4-_DoYgwG"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npWtZAD5X-Hi"
      },
      "source": [
        "def time_segments_aggregate(X, interval, time_column, method=['mean']):\n",
        "    \"\"\"Aggregate values over given time span.\n",
        "    Args:\n",
        "        X (ndarray or pandas.DataFrame):\n",
        "            N-dimensional sequence of values.\n",
        "        interval (int):\n",
        "            Integer denoting time span to compute aggregation of.\n",
        "        time_column (int):\n",
        "            Column of X that contains time values.\n",
        "        method (str or list):\n",
        "            Optional. String describing aggregation method or list of strings describing multiple\n",
        "            aggregation methods. If not given, `mean` is used.\n",
        "    Returns:\n",
        "        ndarray, ndarray:\n",
        "            * Sequence of aggregated values, one column for each aggregation method.\n",
        "            * Sequence of index values (first index of each aggregated segment).\n",
        "    \"\"\"\n",
        "    if isinstance(X, np.ndarray):\n",
        "        X = pd.DataFrame(X)\n",
        "\n",
        "    X = X.sort_values(time_column).set_index(time_column)\n",
        "\n",
        "    if isinstance(method, str):\n",
        "        method = [method]\n",
        "\n",
        "    start_ts = X.index.values[0]\n",
        "    max_ts = X.index.values[-1]\n",
        "\n",
        "    values = list()\n",
        "    index = list()\n",
        "    while start_ts <= max_ts:\n",
        "        end_ts = start_ts + interval\n",
        "        subset = X.loc[start_ts:end_ts - 1]\n",
        "        aggregated = [\n",
        "            getattr(subset, agg)(skipna=True).values\n",
        "            for agg in method\n",
        "        ]\n",
        "        values.append(np.concatenate(aggregated))\n",
        "        index.append(start_ts)\n",
        "        start_ts = end_ts\n",
        "\n",
        "    return np.asarray(values), np.asarray(index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfQE_IlmZv4P"
      },
      "source": [
        "# TimeSegments \n",
        "X, index = time_segments_aggregate(df, interval=1800, time_column='timestamp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49rVid5gYqAI"
      },
      "source": [
        "imp = SimpleImputer()\n",
        "X = imp.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q15IDpMUYyel"
      },
      "source": [
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "X = scaler.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuN0SmzCZ4gx"
      },
      "source": [
        "def rolling_window_sequences(X, index, window_size, target_size, step_size, target_column,\n",
        "                             drop=None, drop_windows=False):\n",
        "    \"\"\"Create rolling window sequences out of time series data.\n",
        "    The function creates an array of input sequences and an array of target sequences by rolling\n",
        "    over the input sequence with a specified window.\n",
        "    Optionally, certain values can be dropped from the sequences.\n",
        "    Args:\n",
        "        X (ndarray):\n",
        "            N-dimensional sequence to iterate over.\n",
        "        index (ndarray):\n",
        "            Array containing the index values of X.\n",
        "        window_size (int):\n",
        "            Length of the input sequences.\n",
        "        target_size (int):\n",
        "            Length of the target sequences.\n",
        "        step_size (int):\n",
        "            Indicating the number of steps to move the window forward each round.\n",
        "        target_column (int):\n",
        "            Indicating which column of X is the target.\n",
        "        drop (ndarray or None or str or float or bool):\n",
        "            Optional. Array of boolean values indicating which values of X are invalid, or value\n",
        "            indicating which value should be dropped. If not given, `None` is used.\n",
        "        drop_windows (bool):\n",
        "            Optional. Indicates whether the dropping functionality should be enabled. If not\n",
        "            given, `False` is used.\n",
        "    Returns:\n",
        "        ndarray, ndarray, ndarray, ndarray:\n",
        "            * input sequences.\n",
        "            * target sequences.\n",
        "            * first index value of each input sequence.\n",
        "            * first index value of each target sequence.\n",
        "    \"\"\"\n",
        "    out_X = list()\n",
        "    out_y = list()\n",
        "    X_index = list()\n",
        "    y_index = list()\n",
        "    target = X[:, target_column]\n",
        "\n",
        "    if drop_windows:\n",
        "        if hasattr(drop, '__len__') and (not isinstance(drop, str)):\n",
        "            if len(drop) != len(X):\n",
        "                raise Exception('Arrays `drop` and `X` must be of the same length.')\n",
        "        else:\n",
        "            if isinstance(drop, float) and np.isnan(drop):\n",
        "                drop = np.isnan(X)\n",
        "            else:\n",
        "                drop = X == drop\n",
        "\n",
        "    start = 0\n",
        "    max_start = len(X) - window_size - target_size + 1\n",
        "    while start < max_start:\n",
        "        end = start + window_size\n",
        "\n",
        "        if drop_windows:\n",
        "            drop_window = drop[start:end + target_size]\n",
        "            to_drop = np.where(drop_window)[0]\n",
        "            if to_drop.size:\n",
        "                start += to_drop[-1] + 1\n",
        "                continue\n",
        "\n",
        "        out_X.append(X[start:end])\n",
        "        out_y.append(target[end:end + target_size])\n",
        "        X_index.append(index[start])\n",
        "        y_index.append(index[end])\n",
        "        start = start + step_size\n",
        "\n",
        "    return np.asarray(out_X), np.asarray(out_y), np.asarray(X_index), np.asarray(y_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3oekRlSZ6Wn"
      },
      "source": [
        "X, y, X_index, y_index = rolling_window_sequences(X, index, \n",
        "                                                  window_size=100, \n",
        "                                                  target_size=1, \n",
        "                                                  step_size=1,\n",
        "                                                  target_column=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhYaWBwpZ-Az"
      },
      "source": [
        "print(\"Training data input shape: {}\".format(X.shape))\n",
        "print(\"Training data index shape: {}\".format(X_index.shape))\n",
        "print(\"Training y shape: {}\".format(y.shape))\n",
        "print(\"Training y index shape: {}\".format(y_index.shape))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjqfKEg9aApL"
      },
      "source": [
        "#### Part 2 \n",
        "\n",
        "- GPU check for TadGAN \n",
        "- Load Tensorflow, Keras, Layers .."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTYvrfHTRUbf"
      },
      "source": [
        "# Check gpu envrionmental \n",
        "import tensorflow as tf\n",
        "import logging\n",
        "import math\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU') \n",
        "if gpus: \n",
        "    try: \n",
        "        for gpu in gpus: \n",
        "            tf.config.experimental.set_memory_growth(gpu, True) \n",
        "    except RuntimeError as e: \n",
        "        print(e)\n",
        "print (gpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8ALUCPXEtM-"
      },
      "source": [
        "LOGGER = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ys30Y68EtM8"
      },
      "source": [
        "#import tensorflow as tf\n",
        "import keras\n",
        "#import similaritymeasures as sm\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Flatten, Dense, Reshape, UpSampling1D, TimeDistributed\n",
        "from tensorflow.keras.layers import Activation, Conv1D, LeakyReLU, Dropout, Add, Layer\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from functools import partial\n",
        "from scipy import integrate, stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR5cPk6ERUbl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyew_Fb5cxid"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coQevcffOvWV"
      },
      "source": [
        "# Model Building 개별 함수화"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj_WD27AOvWV"
      },
      "source": [
        "class RandomWeightedAverage(Layer):\n",
        "    def _merge_function(self, inputs):\n",
        "        alpha = K.random_uniform((64, 1, 1))\n",
        "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDOTybi0OvWV"
      },
      "source": [
        "def build_encoder_layer(input_shape, encoder_reshape_shape):\n",
        "    x = Input(shape=input_shape)\n",
        "    model = tf.keras.models.Sequential([\n",
        "        Bidirectional(LSTM(units=100, return_sequences=True)),\n",
        "        Flatten(),\n",
        "        Dense(20),\n",
        "        Reshape(target_shape=encoder_reshape_shape)])  # (20, 1)\n",
        "\n",
        "    return Model(x, model(x))\n",
        "\n",
        "\n",
        "def build_generator_layer(input_shape, generator_reshape_shape):\n",
        "    # input_shape = (20, 1) / generator_reshape_shape = (50, 1)\n",
        "    x = Input(shape=input_shape)\n",
        "    model = tf.keras.models.Sequential([\n",
        "        Flatten(),\n",
        "        Dense(50),\n",
        "        Reshape(target_shape=generator_reshape_shape),  # (50, 1)\n",
        "        Bidirectional(CuDNNLSTM(units=64, return_sequences=True), merge_mode='concat'),\n",
        "        Dropout(rate=0.2),\n",
        "        UpSampling1D(size=2),\n",
        "        Bidirectional(CuDNNLSTM(units=64, return_sequences=True), merge_mode='concat'),\n",
        "        Dropout(rate=0.2),\n",
        "        TimeDistributed(Dense(1)),\n",
        "        Activation(activation='tanh')])  # (None, 100, 1)\n",
        "\n",
        "    return Model(x, model(x))\n",
        "\n",
        "\n",
        "def build_critic_x_layer(input_shape):\n",
        "    x = Input(shape=input_shape)\n",
        "    model = tf.keras.models.Sequential([\n",
        "        Conv1D(filters=64, kernel_size=5),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dropout(rate=0.25),\n",
        "        Conv1D(filters=64, kernel_size=5),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dropout(rate=0.25),\n",
        "        Conv1D(filters=64, kernel_size=5),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dropout(rate=0.25),\n",
        "        Conv1D(filters=64, kernel_size=5),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dropout(rate=0.25),\n",
        "        Flatten(),\n",
        "        Dense(units=1)])\n",
        "\n",
        "    return Model(x, model(x))\n",
        "\n",
        "\n",
        "def build_critic_z_layer(input_shape):\n",
        "    x = Input(shape=input_shape)\n",
        "    model = tf.keras.models.Sequential([\n",
        "        Flatten(),\n",
        "        Dense(units=100),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dropout(rate=0.2),\n",
        "        Dense(units=100),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dropout(rate=0.2),\n",
        "        Dense(units=1)])\n",
        "\n",
        "    return Model(x, model(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCdHMC3BOvWW"
      },
      "source": [
        "def wasserstein_loss(y_true, y_pred):\n",
        "    return K.mean(y_true * y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8ZcWpI7OvWW"
      },
      "source": [
        "# Layer Parameters\n",
        "latent_dim = 20\n",
        "shape = (100, 1)\n",
        "\n",
        "encoder_input_shape = (100,1)\n",
        "generator_input_shape = (20, 1)\n",
        "critic_x_input_shape = (100,1)\n",
        "critic_z_input_shape = (20,1)\n",
        "encoder_reshape_shape = (20, 1)\n",
        "generator_reshape_shape = (50, 1)\n",
        "learning_rate = 0.0005\n",
        "\n",
        "encoder = build_encoder_layer(input_shape=encoder_input_shape,\n",
        "                              encoder_reshape_shape=encoder_reshape_shape)\n",
        "generator = build_generator_layer(input_shape=generator_input_shape,\n",
        "                                  generator_reshape_shape=generator_reshape_shape)\n",
        "critic_x = build_critic_x_layer(input_shape=critic_x_input_shape)\n",
        "critic_z = build_critic_z_layer(input_shape=critic_z_input_shape)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EPMhkZLOvWW"
      },
      "source": [
        "z = Input(shape=(latent_dim, 1))\n",
        "x = Input(shape=shape)\n",
        "x_ = generator(z)\n",
        "z_ = encoder(x)\n",
        "fake_x = critic_x(x_)\n",
        "valid_x = critic_x(x)\n",
        "interpolated_x = RandomWeightedAverage()([x, x_])\n",
        "#validity_interpolated_x = critic_x(interpolated_x)\n",
        "#partial_gp_loss_x = partial(gradient_penalty_loss, averaged_samples=interpolated_x)\n",
        "#partial_gp_loss_x.__name__ = 'gradient_penalty'\n",
        "\n",
        "critic_x_model = Model(inputs=[x, z], outputs=[valid_x, fake_x, interpolated_x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd-KP-prOvWX"
      },
      "source": [
        "fake_z = critic_z(z_)\n",
        "valid_z = critic_z(z)\n",
        "interpolated_z = RandomWeightedAverage()([z, z_])\n",
        "#validity_interpolated_z = critic_z(interpolated_z)\n",
        "#partial_gp_loss_z = partial(gradient_penalty_loss, averaged_samples=interpolated_z)\n",
        "#partial_gp_loss_z.__name__ = 'gradient_penalty'\n",
        "\n",
        "critic_z_model = Model(inputs=[x, z], outputs=[valid_z, fake_z, interpolated_z])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43QYfgRCOvWX"
      },
      "source": [
        "z_gen = Input(shape=(latent_dim, 1))\n",
        "x_gen_ = generator(z_gen)\n",
        "x_gen = Input(shape=shape)\n",
        "z_gen_ = encoder(x_gen)\n",
        "x_gen_rec = generator(z_gen_)\n",
        "fake_gen_x = critic_x(x_gen_)\n",
        "fake_gen_z = critic_z(z_gen_)\n",
        "\n",
        "encoder_generator_model = Model([x_gen, z_gen], [fake_gen_x, fake_gen_z, x_gen_rec])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp7JdciKOvWX"
      },
      "source": [
        "@tf.function\n",
        "def critic_x_train_on_batch(x, z, valid, fake, delta):\n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "        (valid_x, fake_x, interpolated) = critic_x_model(inputs=[x, z], training=True) \n",
        "        \n",
        "        with tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(interpolated)\n",
        "            pred = critic_x(interpolated, training=True)\n",
        "            \n",
        "        grads = gp_tape.gradient(pred, interpolated)[0]\n",
        "        grads = tf.square(grads)\n",
        "        ddx = tf.sqrt(1e-8 + tf.reduce_sum(grads, axis=np.arange(1, len(grads.shape))))\n",
        "        gp_loss = tf.reduce_mean((ddx - 1.0) ** 2)\n",
        "\n",
        "        loss = tf.reduce_mean(wasserstein_loss(valid, valid_x))\n",
        "        loss += tf.reduce_mean(wasserstein_loss(fake, fake_x))\n",
        "        loss += gp_loss*10.0\n",
        "        \n",
        "    gradients = tape.gradient(loss, critic_x_model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(gradients, critic_x_model.trainable_weights))\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh4QxgIvOvWX"
      },
      "source": [
        "@tf.function\n",
        "def critic_z_train_on_batch(x, z, valid, fake, delta):\n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "        (valid_z, fake_z, interpolated) = critic_z_model(inputs=[x, z], training=True)\n",
        "\n",
        "        with tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(interpolated)\n",
        "            pred = critic_z(interpolated, training=True)\n",
        "            \n",
        "        grads = gp_tape.gradient(pred, interpolated)[0]\n",
        "        grads = tf.square(grads)\n",
        "        ddx = tf.sqrt(1e-8 + tf.reduce_sum(grads, axis=np.arange(1, len(grads.shape))))\n",
        "        gp_loss = tf.reduce_mean((ddx - 1.0) ** 2)\n",
        "\n",
        "        loss = tf.reduce_mean(wasserstein_loss(valid, valid_z))\n",
        "        loss += tf.reduce_mean(wasserstein_loss(fake, fake_z))\n",
        "        loss += gp_loss*10.0        \n",
        "        \n",
        "    gradients = tape.gradient(loss, critic_z_model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(gradients, critic_z_model.trainable_weights))\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rkpv6veOvWY"
      },
      "source": [
        "@tf.function\n",
        "def enc_gen_train_on_batch(x, z, valid):\n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "        (fake_gen_x, fake_gen_z, x_gen_rec) = encoder_generator_model(inputs=[x, z], training=True)\n",
        "        \n",
        "        x = tf.squeeze(x)\n",
        "        x_gen_rec = tf.squeeze(x_gen_rec)\n",
        "        \n",
        "        loss = tf.reduce_mean(wasserstein_loss(valid, fake_gen_x))\n",
        "        loss += tf.reduce_mean(wasserstein_loss(valid, fake_gen_z))\n",
        "        loss += tf.keras.losses.MSE(x, x_gen_rec)*10\n",
        "        loss = tf.reduce_mean(loss)\n",
        "        \n",
        "    gradients = tape.gradient(loss, encoder_generator_model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(gradients, encoder_generator_model.trainable_weights))\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucvXt9XuOvWZ"
      },
      "source": [
        "# Train parameters\n",
        "batch_size = 64\n",
        "n_critics = 5\n",
        "epochs = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ7EjnR4OvWZ"
      },
      "source": [
        "# Train \n",
        "X = X.reshape((-1, shape[0], 1))\n",
        "X_ = np.copy(X)\n",
        "\n",
        "fake = np.ones((batch_size, 1), dtype=np.float32)\n",
        "valid = -np.ones((batch_size, 1), dtype=np.float32)\n",
        "delta = np.ones((batch_size, 1), dtype=np.float32)\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    \n",
        "    np.random.shuffle(X_)\n",
        "    \n",
        "    epoch_g_loss = []\n",
        "    epoch_cx_loss = []\n",
        "    epoch_cz_loss = []\n",
        "    \n",
        "    minibatches_size = batch_size * n_critics\n",
        "    num_minibatches = int(X_.shape[0] // minibatches_size)\n",
        "    \n",
        "    for i in range(num_minibatches):\n",
        "        minibatch = X_[i * minibatches_size: (i + 1) * minibatches_size]\n",
        "        \n",
        "        generator.trainable = False\n",
        "        encoder.trainable = False\n",
        "        # train critics \n",
        "        \n",
        "        for j in range(n_critics):\n",
        "            x = minibatch[j * batch_size: (j + 1) * batch_size]\n",
        "            z = np.random.normal(size=(batch_size, latent_dim, 1))\n",
        "            epoch_cx_loss.append(critic_x_train_on_batch(x, z, valid, fake, delta))\n",
        "            epoch_cz_loss.append(critic_z_train_on_batch(x, z, valid, fake, delta))\n",
        "            \n",
        "        critic_x.trainable = False\n",
        "        critic_z.trainable = False        \n",
        "        generator.trainable = True\n",
        "        encoder.trainable = True        \n",
        "        # train encoder, generator   \n",
        "        \n",
        "        epoch_g_loss.append(enc_gen_train_on_batch(x, z, valid))\n",
        "    \n",
        "    cx_loss = np.mean(np.array(epoch_cx_loss), axis=0)\n",
        "    cz_loss = np.mean(np.array(epoch_cz_loss), axis=0)\n",
        "    g_loss = np.mean(np.array(epoch_g_loss), axis=0)\n",
        "    print('Epoch: {}/{}, [Dx loss: {}] [Dz loss: {}] [G loss: {}]'.format(epoch, epochs, cx_loss, cz_loss, g_loss))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4LOxxEKOvWb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}